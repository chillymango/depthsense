{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a52602305d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from dpt import DepthSense\n",
    "from util.loss import GeoNetLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88408846a7c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(7643)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391a4ebee594781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthSenseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Utility and wrapper for loading datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str):\n",
    "        self.root_dir: Path = Path(root_dir)\n",
    "        #self.directories: list[str] = [\n",
    "        #    d for d in self.root_dir.iterdir() if d.is_dir()\n",
    "        #]\n",
    "\n",
    "    def __getitem__(self, i: int) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        #cur: Path = self.directories[i]\n",
    "        # HACK: google drive mount is super slow...\n",
    "        curr_idx = str(i).rjust(7, '0')\n",
    "        cur = os.path.join(self.root_dir, curr_idx)\n",
    "        image: Tensor = torch.from_numpy(np.load(f\"{cur}/frame.npy\")).float()\n",
    "        depth: Tensor = torch.from_numpy(np.load(f\"{cur}/depth.npy\")).float()\n",
    "        normal: Tensor = torch.from_numpy(np.load(f\"{cur}/normal.npy\")).float()\n",
    "        return image, depth, normal\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 44924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3039c62395938d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and hyperparameters used for training.\n",
    "description: str = \"DepthSense for Metric Depth and Normal Estimation\"\n",
    "model_path: str = \"models/teacher_{}.pth\"\n",
    "\n",
    "batch_size: int = 4\n",
    "betas: tuple[float, float] = 0.9, 0.999\n",
    "dataset_name: Dataset = \"hypersim\"\n",
    "decay: float = 1e-2\n",
    "device: str = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "encoder: str = \"vits\"\n",
    "epochs: int = 10\n",
    "eps: float = 1e-8\n",
    "features: int = 128\n",
    "lr: float = 1e-4\n",
    "refine_edges: bool = False\n",
    "\n",
    "# Data splitting.\n",
    "dataset: Dataset = DepthSenseDataset(f\"datasets/{dataset_name}\")\n",
    "data_size: int = len(dataset)\n",
    "train_size: int = int(0.9 * data_size)\n",
    "val_size: int = data_size - train_size\n",
    "train_set, val_set = data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Model initialization.\n",
    "model_name: str = model_path.replace(\"{}\", dataset_name)\n",
    "model: DepthSense = DepthSense(encoder, features, device=device)\n",
    "criterion: Module = GeoNetLoss()\n",
    "optimizer: Optimizer = AdamW(model.parameters(), lr, betas, eps, decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da570da5e0536eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL. Model loading, if not training from scratch.\n",
    "try:\n",
    "    model = torch.load(model_name, weights_only=True)\n",
    "except FileNotFoundError:\n",
    "    model = DepthSense(encoder, features)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540c7531b6117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training.\n",
    "model.train()\n",
    "\n",
    "train_loader: DataLoader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "max_iters: int = len(train_loader)\n",
    "for e in range(epochs):\n",
    "    running_loss: float = 0.0\n",
    "    for i, (x, z_gt, n_gt) in enumerate(train_loader):\n",
    "        # Move to appropriate device.\n",
    "        x = x.to(device).permute(0, 3, 1, 2)\n",
    "        z_gt = z_gt.to(device)\n",
    "        n_gt = n_gt.to(device)\n",
    "        # Forward pass.\n",
    "        z_hat, n_hat = model(x, refine_edges)\n",
    "        loss: Tensor = criterion(z_hat, z_gt, n_hat, n_gt)\n",
    "        # Backward pass.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Statistics recollection and display.\n",
    "        running_loss += loss.item()\n",
    "        #if (i + 1) % 100 == 0:\n",
    "        loss = running_loss / i\n",
    "        print(f\"Epoch {e}, iter: {i + 1} -- Loss: {loss:.3f}\")\n",
    "\n",
    "# Save current model.\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a777bcabf67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation.\n",
    "model.eval()\n",
    "\n",
    "val_loader: DataLoader = DataLoader(val_set, batch_size, shuffle=True)\n",
    "max_iters: int = len(val_loader)\n",
    "running_loss: float = 0.0\n",
    "for i, (x, z_gt, n_gt) in enumerate(val_loader):\n",
    "    # Move to appropriate device.\n",
    "    x = x.to(device)\n",
    "    z_gt = z_gt.to(device)\n",
    "    n_gt = n_gt.to(device)\n",
    "    # Forward pass.\n",
    "    z, n, z_hat, n_hat = model(x, refine_edges)\n",
    "    loss: Tensor = criterion(z_hat, z_gt, n_hat, n_gt)\n",
    "    running_loss += loss.item()\n",
    "\n",
    "# Statistics recollection and display.\n",
    "avg_loss: float = running_loss / max_iters\n",
    "print(f\"Validation loss: {avg_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f4cbd1a419b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pending tasks.\n",
    "\n",
    "# TODO: Train the teacher and distillate to students."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
